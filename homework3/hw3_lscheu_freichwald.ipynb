{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_lscheu_freichwald.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfpMdIn+9sZBDDgOnZD9zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lscheu2806/iannwtf/blob/main/homework3/hw3_lscheu_freichwald.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZEJS7N9Zv1K"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHsUYKyhaFNu"
      },
      "source": [
        "train_data = tfds.load(\"genomics_ood\", split= \"train[:10%]\")\n",
        "test_data = tfds.load(\"genomics_ood\", split = \"test[:1%]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPOEg--FaPCj"
      },
      "source": [
        "def onehotify(tensor):\n",
        "  vocab = {'A':'1', 'C': '2', 'G':'3', 'T':'0'}\n",
        "  for key in vocab.keys():\n",
        "    tensor = tf.strings.regex_replace(tensor, key, vocab[key])\n",
        "  split = tf.strings.bytes_split(tensor)\n",
        "  labels = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
        "  onehot = tf.one_hot(labels, 4)\n",
        "  onehot = tf.reshape(onehot, (-1,))\n",
        "  return onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCaxhT-HbWfC"
      },
      "source": [
        "train_data_onehot = train_data.map(lambda tensor : onehotify(tensor['seq']))\n",
        "train_data_onehot = train_data_onehot.batch(256)\n",
        "# train_data_onehot = train_data_onehot.shuffle(buffer_size = 256)\n",
        "test_data_onehot = test_data.map(lambda tensor : onehotify(tensor[\"seq\"]))\n",
        "test_data_onehot = train_data_onehot.batch(256)\n",
        "# test_data_onehot = train_data_onehot.shuffle(buffer_size = 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huf2ISQGiIBL"
      },
      "source": [
        "class DenseNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DenseNN, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(256, activation= tf.nn.sigmoid)\n",
        "    self.dense2 = tf.keras.layers.Dense(256, activation= tf.nn.sigmoid)\n",
        "    self.out = tf.keras.layers.Dense(10, activation= tf.nn.softmax)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ-Dzyk3n_mf"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss \n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = np.mean(test_loss_aggregator)\n",
        "  test_accuracy = np.mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWmeXJqSk290",
        "outputId": "5ca2e491-2735-4f55-8cdf-01e58e6b7777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.1\n",
        "running_average_factor = 0.95\n",
        "\n",
        "# Initialize the model.\n",
        "model = DenseNN()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: Adam with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_data_onehot, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_data_onehot, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch: __ ' + str(epoch))\n",
        "\n",
        "    train_data_onehot = train_dataset.shuffle(buffer_size=256)\n",
        "    test_data_onehot = test_dataset.shuffle(buffer_size=256)\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    running_average = 0\n",
        "    for (input,target) in train_data_onehot:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        running_average = running_average_factor * running_average  + (1 - running_average_factor) * train_loss\n",
        "    train_losses.append(running_average)\n",
        "\n",
        "    #testing\n",
        "    test_loss, test_accuracy = test(model, test_data_onehot, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b4e930317cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#testing once before we begin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-cd928018b184>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtest_loss_aggregator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msample_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n7xodveoY0X"
      },
      "source": [
        "# Visualize accuracy and loss for training and test data. \n",
        "# One plot training and test loss.\n",
        "# One plot training and test accuracy.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "line1, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}